{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import embedding_comparison as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_glove = comp.glove_to_dict('./glove.6B.300d.txt')\n",
    "\n",
    "#load only words in common with glove embedding for memory constraints\n",
    "full_current = comp.glove_to_dict('./glove.42B.300d.txt', full_glove.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equalize vocabularies\n",
    "In order to rotate one matrix into the other the have to have the same dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "redux_current, redux_glove = comp.equalize_voc(full_current, full_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotate embeddings\n",
    "Finding the rotation matrix that rotates one embeddings closest to the other reduces to the [orthogonal Procrustes problem](https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rotate current into GloVe\n",
    "rot_current = comp.rotate_embeddings(redux_current, redux_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test how rotation affects the angles between words of different embeddings\n",
    "The rotation should shrink the cosine distances between the same word in the two embeddings.  \n",
    "Then, a set of words that contains verbs, which convey the realization of an event (as described in the event detection [guidelines](https://github.com/dhfbk/current/blob/master/Guidelines.pdf)), some common words, and words whose meaning is supposed to have evolved or changed during time. Common words are taken as a baseline to measure the distance between other words in the different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man woman Current: 0.19520073596550114 GloVe: 0.3001336620380982\n",
      "man after rot: 0.1778400167634343 before rot: 0.9667149934641015\n",
      "he she Current: 0.13041001186124335 GloVe: 0.29290269811032577\n",
      "he after rot: 0.18513043737372248 before rot: 1.036213448955623\n",
      "and but Current: 0.24316111263086237 GloVe: 0.41867866286983835\n",
      "and after rot: 0.19931965724107548 before rot: 0.881821337280183\n"
     ]
    }
   ],
   "source": [
    "ws = [('man', 'woman'), ('he', 'she'), ('and',  'but')]\n",
    "for w1, w2 in ws:\n",
    "    print(w1, w2, 'Current:', comp.cos_dist(rot_current[w1], rot_current[w2]), 'GloVe:', comp.cos_dist(full_glove[w1], full_glove[w2]))\n",
    "    print(w1, 'after rot:', comp.cos_dist(rot_current[w1], full_glove[w1]),\n",
    "          'before rot:', comp.cos_dist(redux_current[w1], full_glove[w1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed_words = ['is', 'be', 'was', 'were', 'do', 'does', 'did', 'done', 'make', 'makes', 'made',\n",
    "                  'get', 'gets', 'got', 'gotten', 'have', 'has', 'had', 'sex', 'keyboard', 'walk',\n",
    "                  'computer', 'airplane', 'gun', 'hotel', 'and', 'but', 'fame', 'sport']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do', 0.12519744733474436),\n",
       " ('make', 0.15295521940549617),\n",
       " ('does', 0.15764962647936975),\n",
       " ('be', 0.1602329575641891),\n",
       " ('done', 0.1652218381482874),\n",
       " ('have', 0.16911801843482643),\n",
       " ('had', 0.17258309920704373),\n",
       " ('computer', 0.174469882834433),\n",
       " ('did', 0.17654384206506335),\n",
       " ('get', 0.18270704460419185),\n",
       " ('made', 0.18309746638086644),\n",
       " ('is', 0.18410232929541437),\n",
       " ('but', 0.18767834844705222),\n",
       " ('has', 0.1919236824880206),\n",
       " ('were', 0.19908112092800978),\n",
       " ('and', 0.19931965724107548),\n",
       " ('got', 0.20079406143094547),\n",
       " ('sport', 0.21013923448358474),\n",
       " ('fame', 0.21854641266759078),\n",
       " ('was', 0.2190861711704336),\n",
       " ('airplane', 0.22354896589894446),\n",
       " ('makes', 0.22436744577893575),\n",
       " ('gun', 0.2261838567033444),\n",
       " ('walk', 0.2294932747049221),\n",
       " ('gotten', 0.23136626027539997),\n",
       " ('gets', 0.23601584524410513),\n",
       " ('keyboard', 0.2550318857457389),\n",
       " ('hotel', 0.30678723712006517),\n",
       " ('sex', 0.3097621064840107)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(w, comp.cos_dist(rot_current[w], full_glove[w])) for w in analyzed_words],\n",
    "       key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN search\n",
    "The previous analysis is compared against a different technique to measure words distance in two different embeddings to see weather the produce similar results. Given a word the set of K nearest neighbour of that word is taken. This is done for each embedding, then the Jaccard distance between the resulting sets is computed. The analysis is done both with equal and original vocabularies for the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do', 0.31932773109243695),\n",
       " ('make', 0.360655737704918),\n",
       " ('does', 0.4126984126984127),\n",
       " ('have', 0.4126984126984127),\n",
       " ('but', 0.4126984126984127),\n",
       " ('is', 0.4251968503937008),\n",
       " ('be', 0.4251968503937008),\n",
       " ('get', 0.4375),\n",
       " ('and', 0.4496124031007752),\n",
       " ('did', 0.46153846153846156),\n",
       " ('got', 0.4732824427480916),\n",
       " ('done', 0.48484848484848486),\n",
       " ('makes', 0.5074626865671642),\n",
       " ('made', 0.5074626865671642),\n",
       " ('had', 0.5074626865671642),\n",
       " ('airplane', 0.5294117647058824),\n",
       " ('was', 0.5401459854014599),\n",
       " ('has', 0.5401459854014599),\n",
       " ('were', 0.5611510791366907),\n",
       " ('gets', 0.5611510791366907),\n",
       " ('gotten', 0.5815602836879432),\n",
       " ('computer', 0.6206896551724138),\n",
       " ('walk', 0.6301369863013699),\n",
       " ('fame', 0.6394557823129252),\n",
       " ('keyboard', 0.6842105263157895),\n",
       " ('gun', 0.6928104575163399),\n",
       " ('sport', 0.7012987012987013),\n",
       " ('hotel', 0.7261146496815287),\n",
       " ('sex', 0.8505747126436781)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalize vector embeddings so that Cosine distance ~ Euclidean distance\n",
    "#this is done because the data structure for efficient NN search (KDtree) allows only Euclidean distance\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "ncurrent = comp.normalize_embedding(redux_current)\n",
    "nglove = comp.normalize_embedding(redux_glove)\n",
    "\n",
    "#create KDTree representation of embeddings for fast NN search\n",
    "tree_ncurrent = spatial.KDTree(list(ncurrent.values()))\n",
    "\n",
    "tree_nglove = spatial.KDTree(list(nglove.values()))\n",
    "\n",
    "sorted([(w, comp.word_jaccard_distance(w, ncurrent, tree_ncurrent, nglove, tree_nglove)[0]) for w in analyzed_words],\n",
    "       key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inforet",
   "language": "python",
   "name": "inforet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
