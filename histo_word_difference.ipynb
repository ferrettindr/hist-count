{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import embedding_comparison as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_histo = comp.glove_to_dict('./HistoGlove.txt')\n",
    "full_glove = comp.glove_to_dict('./glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equalize vocabularies\n",
    "In order to rotate one matrix into the other the have to have the same dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "redux_histo, redux_glove = comp.equalize_voc(full_histo, full_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotate embeddings\n",
    "Finding the rotation matrix that rotates one embeddings closest to the other reduces to the [orthogonal Procrustes problem](https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rotate Histo into GloVe\n",
    "rot_histo = comp.rotate_embeddings(redux_histo, redux_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test how rotation affects the angles between words of different embeddings\n",
    "The rotation should shrink the cosine distances between the same word in the two embeddings.  \n",
    "Then, a set of words that contains verbs, which convey the realization of an event (as described in the event detection [guidelines](https://github.com/dhfbk/Histo/blob/master/Guidelines.pdf)), some common words, and words whose meaning is supposed to have evolved or changed during time. Common words are taken as a baseline to measure the distance between other words in the different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man woman Histo: 0.3378917570693979 GloVe: 0.3001336620380982\n",
      "man after rot: 0.2560016223450836 before rot: 0.9332433812838266\n",
      "he she Histo: 0.28377190534703045 GloVe: 0.29290269811032577\n",
      "he after rot: 0.18554595423735554 before rot: 1.0279926588039803\n",
      "and but Histo: 0.40557248737786233 GloVe: 0.41867866286983835\n",
      "and after rot: 0.15487231194454842 before rot: 1.039363584463667\n"
     ]
    }
   ],
   "source": [
    "ws = [('man', 'woman'), ('he', 'she'), ('and',  'but')]\n",
    "for w1, w2 in ws:\n",
    "    print(w1, w2, 'Histo:', comp.cos_dist(rot_histo[w1], rot_histo[w2]), 'GloVe:', comp.cos_dist(full_glove[w1], full_glove[w2]))\n",
    "    print(w1, 'after rot:', comp.cos_dist(rot_histo[w1], nglove[w1]),\n",
    "          'before rot:', comp.cos_dist(full_histo[w1], full_glove[w1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed_words = ['is', 'be', 'was', 'were', 'do', 'does', 'did', 'done', 'make', 'makes', 'made',\n",
    "                  'get', 'gets', 'got', 'gotten', 'have', 'has', 'had', 'sex', 'keyboard', 'walk',\n",
    "                  'computer', 'airplane', 'gun', 'hotel', 'and', 'but', 'fame', 'sport']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 0.1548723119445482),\n",
       " ('had', 0.16644375107820275),\n",
       " ('was', 0.18747676698073124),\n",
       " ('have', 0.19247986779105275),\n",
       " ('were', 0.1936423598387852),\n",
       " ('be', 0.19706325709843442),\n",
       " ('is', 0.20375246760367294),\n",
       " ('but', 0.23312515747075713),\n",
       " ('has', 0.24105492103205994),\n",
       " ('made', 0.259986181306759),\n",
       " ('do', 0.26224090087386687),\n",
       " ('got', 0.2630604146022776),\n",
       " ('does', 0.264535503217891),\n",
       " ('did', 0.2649323760645266),\n",
       " ('make', 0.2690426887029076),\n",
       " ('get', 0.2704075653322171),\n",
       " ('hotel', 0.2890237526744279),\n",
       " ('gets', 0.3094831914474211),\n",
       " ('done', 0.3147224944654414),\n",
       " ('makes', 0.3287810561318193),\n",
       " ('walk', 0.3318151115027683),\n",
       " ('gun', 0.4206272499428587),\n",
       " ('airplane', 0.45241414065377494),\n",
       " ('fame', 0.4946716974519002),\n",
       " ('sport', 0.5563455720425707),\n",
       " ('sex', 0.5665992716174372),\n",
       " ('gotten', 0.5848037183813057),\n",
       " ('keyboard', 0.7635135908862496),\n",
       " ('computer', 1.0758251455929164)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(w, comp.cos_dist(rot_histo[w], full_glove[w])) for w in analyzed_words],\n",
    "       key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN search\n",
    "The previous analysis is compared against a different technique to measure words distance in two different embeddings to see weather the produce similar results. Given a word the set of K nearest neighbour of that word is taken. This is done for each embedding, then the Jaccard distance between the resulting sets is computed. The analysis is done both with equal and original vocabularies for the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('did', 0.4126984126984127),\n",
       " ('does', 0.4375),\n",
       " ('do', 0.4496124031007752),\n",
       " ('be', 0.46153846153846156),\n",
       " ('is', 0.4732824427480916),\n",
       " ('make', 0.4732824427480916),\n",
       " ('made', 0.49624060150375937),\n",
       " ('have', 0.49624060150375937),\n",
       " ('had', 0.5294117647058824),\n",
       " ('but', 0.5294117647058824),\n",
       " ('and', 0.5507246376811594),\n",
       " ('done', 0.5714285714285714),\n",
       " ('get', 0.5714285714285714),\n",
       " ('were', 0.5815602836879432),\n",
       " ('got', 0.5815602836879432),\n",
       " ('was', 0.5915492957746479),\n",
       " ('has', 0.5915492957746479),\n",
       " ('makes', 0.6666666666666667),\n",
       " ('walk', 0.7012987012987013),\n",
       " ('gets', 0.7096774193548387),\n",
       " ('hotel', 0.7654320987654322),\n",
       " ('gun', 0.7730061349693251),\n",
       " ('airplane', 0.7878787878787878),\n",
       " ('sport', 0.8304093567251463),\n",
       " ('fame', 0.8372093023255813),\n",
       " ('gotten', 0.9189189189189189),\n",
       " ('sex', 0.9304812834224598),\n",
       " ('keyboard', 0.9847715736040609),\n",
       " ('computer', 0.9949748743718593)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalize vector embeddings so that Cosine distance ~ Euclidean distance\n",
    "#this is done because the data structure for efficient NN search (KDtree) allows only Euclidean distance\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "nhisto = comp.normalize_embedding(redux_histo)\n",
    "nglove = comp.normalize_embedding(redux_glove)\n",
    "\n",
    "#create KDTree representation of embeddings for fast NN search\n",
    "tree_nhisto = spatial.KDTree(list(nhisto.values()))\n",
    "\n",
    "tree_nglove = spatial.KDTree(list(nglove.values()))\n",
    "\n",
    "sorted([(w, comp.word_jaccard_distance(w, nhisto, tree_nhisto, nglove, tree_nglove)[0]) for w in analyzed_words],\n",
    "       key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('did', 0.4126984126984127),\n",
       " ('does', 0.4375),\n",
       " ('do', 0.4496124031007752),\n",
       " ('be', 0.46153846153846156),\n",
       " ('is', 0.4732824427480916),\n",
       " ('make', 0.4732824427480916),\n",
       " ('made', 0.49624060150375937),\n",
       " ('have', 0.49624060150375937),\n",
       " ('had', 0.5294117647058824),\n",
       " ('but', 0.5294117647058824),\n",
       " ('and', 0.5507246376811594),\n",
       " ('done', 0.5714285714285714),\n",
       " ('get', 0.5714285714285714),\n",
       " ('were', 0.5815602836879432),\n",
       " ('got', 0.5815602836879432),\n",
       " ('was', 0.5915492957746479),\n",
       " ('has', 0.5915492957746479),\n",
       " ('makes', 0.6666666666666667),\n",
       " ('walk', 0.7012987012987013),\n",
       " ('gets', 0.7096774193548387),\n",
       " ('hotel', 0.7804878048780488),\n",
       " ('gun', 0.7951807228915663),\n",
       " ('airplane', 0.8095238095238095),\n",
       " ('fame', 0.8372093023255813),\n",
       " ('sport', 0.8636363636363636),\n",
       " ('gotten', 0.9304812834224598),\n",
       " ('sex', 0.9304812834224598),\n",
       " ('keyboard', 0.9847715736040609),\n",
       " ('computer', 0.9949748743718593)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfull_histo = comp.normalize_embedding(full_histo)\n",
    "nfull_glove = comp.normalize_embedding(full_glove)\n",
    "\n",
    "tree_fhisto = spatial.KDTree(list(nfull_histo.values()))\n",
    "\n",
    "tree_fglove = spatial.KDTree(list(nfull_glove.values()))\n",
    "\n",
    "sorted([(w, comp.word_jaccard_distance(w, nfull_histo, tree_fhisto, nfull_glove, tree_fglove)[0]) for w in analyzed_words],\n",
    "       key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inforet",
   "language": "python",
   "name": "inforet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
