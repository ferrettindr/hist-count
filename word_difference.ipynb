{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "import os\n",
    "\n",
    "def glove_to_dict(embedding: str, embedding_length: int = 300) -> dict:\n",
    "    result = dict()\n",
    "    for line in open(embedding, 'r'):\n",
    "        line = line.split()\n",
    "        result[line[0]] = np.array(line[1:]).astype(np.float)\n",
    "    return result\n",
    "\n",
    "def cos_dist(x: str, y: str, emb: dict) -> float:\n",
    "    return spatial.distance.cosine(emb[x], emb[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_histo = glove_to_dict('./HistoGlove.txt')\n",
    "full_glove = glove_to_dict('./glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equalize vocabularies\n",
    "In order to rotate one matrix into the other the have to have the same dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#make the vocabularies equal\n",
    "shisto = set(full_histo.keys())\n",
    "sglove = set(full_glove.keys())\n",
    "\n",
    "#keep only words that appear in both embeddings\n",
    "voc = shisto.intersection(sglove)\n",
    "\n",
    "redux_histo = {k:full_histo[k] for k in voc}\n",
    "redux_glove = {k:full_glove[k] for k in voc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotate embeddings\n",
    "Finding the rotation matrix that rotates one embeddings closest to the other reduces to the [orthogonal Procrustes problem](https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000001128\n"
     ]
    }
   ],
   "source": [
    "#Find the rotation matrix R\n",
    "A = np.array([v for v in redux_histo.values()])\n",
    "B = np.array([v for v in redux_glove.values()])\n",
    "R = scipy.linalg.orthogonal_procrustes(A,B)[0]\n",
    "R.shape\n",
    "\n",
    "M = np.dot(A.T, B)\n",
    "U, s, V = np.linalg.svd(M)\n",
    "U.shape, V.shape, s.shape\n",
    "#determinant == 1, it is a proper rotation matrix (only rotation, no reflection)\n",
    "print(np.linalg.det(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rotate Histo into GloVe\n",
    "rot_histo = {k:v for k,v in zip(voc, A @ R)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test how rotation affects the angles between words of different embeddings\n",
    "The rotation should shrink the cosine distances between the same word in the two embeddings.  \n",
    "Then, a set of words that contains verbs, which convey the realization of an event (as described in the event detection [guidelines](https://github.com/dhfbk/Histo/blob/master/Guidelines.pdf)), some common words, and words whose meaning is supposed to have evolved or changed during time. Common words are taken as a baseline to measure the distance between other words in the different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man woman Histo: 0.337891757069398 GloVe: 0.3001336620380982\n",
      "man after rot: 0.25600162234507673 before rot: 0.9332433812838266\n",
      "he she Histo: 0.28377190534703056 GloVe: 0.29290269811032577\n",
      "he after rot: 0.18554595423735953 before rot: 1.0279926588039803\n",
      "and but Histo: 0.4055724873778621 GloVe: 0.41867866286983835\n",
      "and after rot: 0.15487231194454343 before rot: 1.039363584463667\n"
     ]
    }
   ],
   "source": [
    "ws = [('man', 'woman'), ('he', 'she'), ('and',  'but')]\n",
    "for w1, w2 in ws:\n",
    "    print(w1, w2, 'Histo:', cos_dist(w1, w2, rot_histo), 'GloVe:', cos_dist(w1, w2, full_glove))\n",
    "    print(w1, 'after rot:', spatial.distance.cosine(rot_histo[w1], full_glove[w1]),\n",
    "          'before rot:', spatial.distance.cosine(full_histo[w1], full_glove[w1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed_words = ['is', 'be', 'was', 'were', 'do', 'does', 'did', 'done', 'make', 'makes', 'made',\n",
    "                  'get', 'gets', 'got', 'gotten', 'have', 'has', 'had', 'sex', 'keyboard', 'walk',\n",
    "                  'computer', 'airplane', 'gun', 'hotel', 'and', 'but', 'fame', 'sport']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 0.15487231194454343),\n",
       " ('had', 0.16644375107820242),\n",
       " ('was', 0.1874767669807348),\n",
       " ('have', 0.1924798677910513),\n",
       " ('were', 0.19364235983878308),\n",
       " ('be', 0.19706325709842853),\n",
       " ('is', 0.20375246760367383),\n",
       " ('but', 0.23312515747075813),\n",
       " ('has', 0.2410549210320615),\n",
       " ('made', 0.2599861813067612),\n",
       " ('do', 0.26224090087386576),\n",
       " ('got', 0.2630604146022777),\n",
       " ('does', 0.2645355032178899),\n",
       " ('did', 0.2649323760645279),\n",
       " ('make', 0.2690426887029107),\n",
       " ('get', 0.270407565332216),\n",
       " ('hotel', 0.2890237526744238),\n",
       " ('gets', 0.30948319144741465),\n",
       " ('done', 0.3147224944654413),\n",
       " ('makes', 0.3287810561318143),\n",
       " ('walk', 0.3318151115027739),\n",
       " ('gun', 0.4206272499428617),\n",
       " ('airplane', 0.45241414065377694),\n",
       " ('fame', 0.49467169745189543),\n",
       " ('sport', 0.556345572042569),\n",
       " ('sex', 0.5665992716174489),\n",
       " ('gotten', 0.5848037183813042),\n",
       " ('keyboard', 0.7635135908862524),\n",
       " ('computer', 1.07582514559291)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(w, spatial.distance.cosine(rot_histo[w], full_glove[w])) for w in analyzed_words],\n",
    "       key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN search\n",
    "The previous analysis is compared against a different technique to measure words distance in two different embeddings to see weather the produce similar results. Given a word the set of K nearest neighbour of that word is taken. This is done for each embedding, then the Jaccard distance between the resulting sets is computed. The analysis is done both with equal and original vocabularies for the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize vector embeddings so that Cosine distance ~ Euclidean distance\n",
    "#this is done because the data structure for efficient NN search (KDtree) allows only Euclidean distance\n",
    "def normalize_embedding(emb: dict) -> dict:\n",
    "    return {w:emb[w]/np.linalg.norm(emb[w]) for w in emb}\n",
    "\n",
    "nhisto = normalize_embedding(redux_histo)\n",
    "nglove = normalize_embedding(redux_glove)\n",
    "nfull_histo = normalize_embedding(full_histo)\n",
    "nfull_glove = normalize_embedding(full_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create KDTree representation of embeddings for fast NN search\n",
    "tree_nhisto = spatial.KDTree(list(nhisto.values()))\n",
    "\n",
    "tree_nglove = spatial.KDTree(list(nglove.values()))\n",
    "\n",
    "tree_fhisto = spatial.KDTree(list(nfull_histo.values()))\n",
    "\n",
    "tree_fglove = spatial.KDTree(list(nfull_glove.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't remove words meaningful for the analysis\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) - set(analyzed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Jaccard distance of the selected word\n",
    "def word_jaccard_distance(word: str, emb1: dict, emb1_tree: spatial.KDTree,\n",
    "                          emb2: dict, emb2_tree: spatial.KDTree, k: int=50) -> tuple:\n",
    "    dist_emb1, neigh_emb1 = emb1_tree.query(emb1[word], k=k)\n",
    "    res_emb1 = [list(emb1.keys())[i] for i in neigh_emb1]\n",
    "    \n",
    "    dist_emb2, neigh_emb2 = emb2_tree.query(emb2[word], k=k)\n",
    "    res_emb2 = [list(emb2.keys())[i] for i in neigh_emb2]\n",
    "    \n",
    "    s1, s2 = set(res_emb1), set(res_emb2)\n",
    "    dist = 1 - len(s1.intersection(s2))/len(s1.union(s2))\n",
    "    \n",
    "    return dist , dict(zip(res_emb1, dist_emb1)), dict(zip(res_emb2, dist_emb2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 0.48484848484848486),\n",
       " ('do', 0.48484848484848486),\n",
       " ('get', 0.48484848484848486),\n",
       " ('were', 0.5074626865671642),\n",
       " ('is', 0.5294117647058824),\n",
       " ('done', 0.5294117647058824),\n",
       " ('has', 0.5294117647058824),\n",
       " ('but', 0.5294117647058824),\n",
       " ('was', 0.5507246376811594),\n",
       " ('does', 0.5507246376811594),\n",
       " ('did', 0.5507246376811594),\n",
       " ('make', 0.5507246376811594),\n",
       " ('have', 0.5915492957746479),\n",
       " ('and', 0.5915492957746479),\n",
       " ('made', 0.6111111111111112),\n",
       " ('makes', 0.6301369863013699),\n",
       " ('got', 0.6666666666666667),\n",
       " ('gets', 0.6842105263157895),\n",
       " ('had', 0.6842105263157895),\n",
       " ('gun', 0.717948717948718),\n",
       " ('walk', 0.75),\n",
       " ('airplane', 0.75),\n",
       " ('hotel', 0.7804878048780488),\n",
       " ('fame', 0.8505747126436781),\n",
       " ('sport', 0.8764044943820225),\n",
       " ('sex', 0.9130434782608696),\n",
       " ('gotten', 0.9247311827956989),\n",
       " ('keyboard', 0.9690721649484536),\n",
       " ('computer', 0.98989898989899)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(w, word_jaccard_distance(w, nhisto, tree_nhisto, nglove, tree_nglove)[0]) for w in analyzed_words],\n",
    "       key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inforet",
   "language": "python",
   "name": "inforet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
